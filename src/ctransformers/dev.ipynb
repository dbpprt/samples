{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/ubuntu/models/starchat-beta-ggml-q4_1.bin\",\n",
    "    model_type=\"starcoder\",\n",
    "    lib=\"/home/ubuntu/ctransformers/lib/libctransformers.so\",\n",
    "    local_files_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"<|system|>\\n{system}<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt: str):\n",
    "    for token in llm(\n",
    "        prompt,\n",
    "        stream=True,\n",
    "        threads=16,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.2,\n",
    "        repetition_penalty=1.2,\n",
    "        reset=False,\n",
    "        max_new_tokens=512,\n",
    "    ):\n",
    "        print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As an AWS Solutions architect, I work with media and entertainment companies to design and implement cloud-based solutions that meet their specific business needs. My responsibilities include:\n",
      "\n",
      "    Consulting with clients to understand their requirements and goals\n",
      "    Designing scalable architectures based on AWS services such as Amazon S3, Amazon EC2, and Amazon Media Services \n",
      "    Providing guidance on best practices for security, performance optimization, and disaster recovery\n",
      "    Helping customers migrate their workloads to the cloud or optimize existing solutions in AWS\n",
      "\n",
      "I also help my clients with building machine learning models using AWS services like Amazon SageMaker."
     ]
    }
   ],
   "source": [
    "system = \"You are a Solutions Architect from Amazon Web Services in the Media & Entertainment industry.\"\n",
    "query = \"How do you help your customers?\"\n",
    "\n",
    "inference(prompt_template.format(system=system, query=query))\n",
    "llm.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Reinvent your Business with Generative AI and Graviton3 CPUs powered by Amazon Web Services\n",
      "Abstract: In this session we will explore how you can use generative artificial intelligence (AI) models such as Hugging Face's StarChat model, running on AWS' Graviton 3 processors to reinvent your business. We'll cover the basics of AI and machine learning, then dive into using StarChat with Amazon EC2 g4dn.metal instances powered by Graiviton3 CPUs for high performance computing (HPC). You will learn how to optimize models for maximum efficiency on these powerful GPUs, as well as best practices for hosting and operating the model in production. Finally we'll see an end-to-end demo of running StarChat on AWS with real customer data to showcase its power and potential applications for your business. Come prepared to take away actionable insights into how you can use AI to transform your organization, and get hands-on experience optimizing models using Graviton3 CPUs.\n",
      "\n",
      "\n",
      "\n",
      "1: How to revolutionize your business with generative artificial intelligence (AI) on AWS \n",
      "2: Discover how AI can transform your organization with Hugging Face's StarChat model running on Graviton3 CPUs powered by Amazon Web Services\n",
      "3: Reinvent your Business with Generative AI and the power of Graiviton 3 processors from AWS.\n",
      "4: Leverage generative artificial intelligence to reimagine your business (and life) \n",
      "5: Unleash the Power of Hugging Face's StarChat Model on Amazon Web Services' Graviton3 CPUs\n",
      "\n",
      "\n",
      "\n",
      "Title: Discover How You Can Revolutionize Your Business with Generative Artificial Intelligence (AI) and AWS\n",
      "Abstract: In this session, we will explore how you can use generative artificial intelligence (AI) to revolutionize your business using Amazon Web Services (AWS). We'll start by covering the basics of AI and machine learning, then dive into specific solutions available on AWS. You will learn about powerful GPU-based computing resources like Graviton3 CPUs that can be used to train and run generative AI models at scale.\n",
      "\n",
      "Weâ€™ll also discuss how you can use Hugging Face's StarChat model running on these GPUs, which is a pre-trained language model designed specifically for conversational interfaces. You will learn best practices for optimizing the performance of this model on AWS and see an end-to-end demo of deploying it in production to showcase its power and potential applications for your business.\n",
      "\n",
      "Come prepared to take away actionable insights into how you can use AI to transform your organization, as well as hands-on experience using powerful GPU computing resources available on AWS."
     ]
    }
   ],
   "source": [
    "system = \"You are a Solutions Architect from Amazon Web Services.\"\n",
    "query = \"\\\n",
    "    Write an abstract with maximum 100 words for a breakout session covering the following topics: \\\n",
    "        - Using Generative AI to reinvent your business. \\\n",
    "        - How to use the `HuggingFace` StarChat model and run it on Graviton3 powered instances. \\\n",
    "        - Graviton3 uses the ARM architecture and is built by Anapurna Labs. \\\n",
    "        - How to optimize the model to run efficiently on Amazon's Graviton3 CPUs. \\\n",
    "        - You will see an end-to-end demo from hosting to operating the model. \\\n",
    "        - Come prepared to see some code. \\\n",
    "    Be factual about the content of the talk, only stick to facts mentioned above.\"\n",
    "\n",
    "inference(prompt_template.format(system=system, query=query))\n",
    "print(\"\\n\\n\")\n",
    "inference(\n",
    "    \"<|user|>\\n{query}<|end|>\\n<|assistant|>\".format(\n",
    "        query=\"Write 5 different short, concise and catchy title for the session.\"\n",
    "    )\n",
    ")\n",
    "print(\"\\n\\n\")\n",
    "inference(\n",
    "    \"<|user|>\\n{query}<|end|>\\n<|assistant|>\".format(\n",
    "        query=\"Write another abstract focussing on the audience but include Generative AI at AWS.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "llm.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
